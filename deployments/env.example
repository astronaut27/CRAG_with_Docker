# ============================================================================
# Mock API Configuration (for RAG data)
# ============================================================================
# If running mock API locally, use localhost
# If running mock API in Docker, use mock-api:8000
CRAG_MOCK_API_URL=http://mock-api:8000

# ============================================================================
# RAG Model Configuration (for answer generation)
# ============================================================================
# Configuration for the model that generates answers (Ollama/LM Studio)
# For Ollama: http://localhost:11434/v1
# For LM Studio: http://localhost:1234/v1
RAG_MODEL_API_BASE=http://localhost:11434/v1
RAG_MODEL_API_KEY=ollama
RAG_MODEL_NAME=llama3
RAG_MODEL_BATCH_SIZE=4

# ============================================================================
# Evaluation Configuration (LLM as Judge)
# ============================================================================
# Flag to determine evaluation method:
#   true  = Use paid OpenAI API for evaluation (default)
#   false = Use local judge (LM Studio/Ollama) for evaluation
EVALUATION_OPENAPI=true

# ----------------------------------------------------------------------------
# Paid OpenAI API Configuration (when EVALUATION_OPENAPI=true)
# ----------------------------------------------------------------------------
# Get your API key from https://platform.openai.com/api-keys
OPENAPI_API_KEY=sk-your-openai-key-here
OPENAPI_MODEL_NAME=gpt-4-0125-preview

# ----------------------------------------------------------------------------
# Local Judge Configuration (when EVALUATION_OPENAPI=false)
# ----------------------------------------------------------------------------
# For LM Studio: http://localhost:1234/v1
# For Ollama: http://localhost:11434/v1
LOCAL_JUDGE_API_BASE=http://localhost:1234/v1
LOCAL_JUDGE_API_KEY=lm-studio
LOCAL_JUDGE_MODEL_NAME=llama3
